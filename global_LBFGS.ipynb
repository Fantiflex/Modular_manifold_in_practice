{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fantiflex/Modular_Manifold_Muon/blob/main/global_LBFGS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFsIyq9fH8df"
      },
      "source": [
        "**Setup**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxUtCGhTH4VC"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymcDnaqFIbbf"
      },
      "source": [
        "**L-BFGS optimizer class setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqrjwWZvIa-X"
      },
      "outputs": [],
      "source": [
        "class ManifoldLBFGS:\n",
        "  \"\"\"\n",
        "    Riemannian L-BFGS sur Stiefel avec:\n",
        "      - métrique euclidienne (produit de Frobenius)\n",
        "      - rétraction polaire (msign/polar)\n",
        "      - transport par projection\n",
        "    Utilisation typique:\n",
        "        opt = ManifoldLBFGS(eta=0.1, history=10)\n",
        "        # itération k\n",
        "        W = opt.step(W, Gk)           # prend un pas, renvoie W_{k+1}\n",
        "        ...\n",
        "        Gkp1 = ...                    # gradient au nouveau point (fourni par toi)\n",
        "        opt.update(Gkp1)              # met à jour l'historique (s_k, y_k)\n",
        "    \"\"\"\n",
        "  def __init__(self, eta=0.1, history=10,eps_curv=1e-12, c0=1e-4, c1=1.0, c2=None):\n",
        "    '''\n",
        "    W : parameters\n",
        "    G : gradient\n",
        "    opt : instance optimiseur\n",
        "    '''\n",
        "    self.eta = eta\n",
        "    self.history = history\n",
        "    self.c0, self.c1 = c0, c1\n",
        "    self.m=history\n",
        "    self.c2 = (1.0/(2*history+3)) if c2 is None else c2\n",
        "    self.pairs = []                 # liste (s_i, y_i) dans le tangent courant\n",
        "    self._pending = None            # stocke (W_old, G_old_riem, eta_vec) entre step() et update()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # two-loop uses stored (s_i, y_i) in current tangent; H0 = gamma*I\n",
        "  def two_loop(self, G, pairs, gamma, eps=1e-16):\n",
        "    q = G.clone()\n",
        "    alphas = []\n",
        "    for s, y in reversed(pairs):\n",
        "        ys = (y.flatten() @ s.flatten()).item()\n",
        "        a  = (s.flatten() @ q.flatten()).item() / (ys + eps)\n",
        "        alphas.append(a)\n",
        "        q = q - a * y\n",
        "    r = gamma * q\n",
        "    for (s, y), a in zip(pairs, reversed(alphas)):\n",
        "        ys = (y.flatten() @ s.flatten()).item()\n",
        "        b  = (y.flatten() @ r.flatten()).item() / (ys + eps)\n",
        "        r = r + (a - b) * s\n",
        "    return -r\n",
        "\n",
        "  # une étape RL-BFGS \"Mannel\"\n",
        "  def step(self, W, G):               # <--- plus de closure ici\n",
        "        Griem = tangent_proj(W, G)\n",
        "        gamma = self._gamma_from_memory(default=1.0)\n",
        "        P = self.two_loop(Griem, self.pairs, gamma)\n",
        "\n",
        "        # Pas simple sans line search ; option : normaliser la longueur\n",
        "        step_vec = (self.eta * P) if P.norm() == 0 else (self.eta * P / (P.norm() + 1e-16))\n",
        "        W_new = retract_stiefel_shape_preserving(W,step_vec)\n",
        "\n",
        "\n",
        "        # garder info pour construire (s,y) au prochain update()\n",
        "        self._pending = (W.detach(), Griem.detach(), step_vec.detach(), W_new.detach())\n",
        "        return W_new\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def update(self, G_new):            # appeler APRES avoir recomputé le gradient au nouveau W\n",
        "      if self._pending is None:\n",
        "          return\n",
        "      W_old, G_old_riem, step_vec, W_new = self._pending\n",
        "      # transport projeté\n",
        "      s = transport_proj(W_old, W_new, step_vec)\n",
        "      G_new_riem = tangent_proj(W_new, G_new)\n",
        "      y = G_new_riem - transport_proj(W_old, W_new, G_old_riem)\n",
        "\n",
        "      omega = min(self.c0, self.c1 * (G_old_riem.norm().item() ** self.c2))\n",
        "      sy = (s.flatten() @ y.flatten()).item()\n",
        "      if sy >= omega * max(s.norm().pow(2).item(), y.norm().pow(2).item()):\n",
        "          self.pairs.append((s.detach(), y.detach()))\n",
        "          if len(self.pairs) > self.history:\n",
        "              self.pairs.pop(0)\n",
        "\n",
        "      # transporter toute la mémoire dans le nouveau tangent\n",
        "      self.pairs = [(transport_proj(W_old, W_new, si),\n",
        "                      transport_proj(W_old, W_new, yi)) for (si, yi) in self.pairs]\n",
        "      self._pending = None\n",
        "\n",
        "  def _gamma_from_memory(self, default=1.0):\n",
        "      if not self.pairs: return default\n",
        "      s, y = self.pairs[-1]\n",
        "      sty = (s.flatten() @ y.flatten()).item()\n",
        "      yy  = (y.flatten() @ y.flatten()).item()\n",
        "      if sty > 0:   g = sty / (yy + 1e-16)\n",
        "      elif yy > 0:  g = (s.norm() / (y.norm() + 1e-16)).item()\n",
        "      else:         g = default\n",
        "      omega = min(self.c0, self.c1 * 1.0 ** self.c2)\n",
        "      return max(omega, min(g, 1.0/omega))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKuMrrQqVhTw"
      },
      "source": [
        "**Useful geometrical riemaniann functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9je4DdNYr7d"
      },
      "outputs": [],
      "source": [
        "def _as_matrix(X):\n",
        "    \"\"\"\n",
        "    View X as a 2D matrix (rows, cols) by collapsing all dims except the first.\n",
        "    Returns (X_mat, orig_shape) so we can reshape back afterwards.\n",
        "    \"\"\"\n",
        "    orig_shape = X.shape\n",
        "    if X.ndim == 2:\n",
        "        return X, orig_shape\n",
        "    X_mat = X.reshape(X.shape[0], -1)\n",
        "    return X_mat, orig_shape\n",
        "\n",
        "\n",
        "def _from_matrix(X_mat, orig_shape):\n",
        "    \"\"\"\n",
        "    Reshape a 2D matrix back to the original tensor shape.\n",
        "    Assumes first dimension is unchanged.\n",
        "    \"\"\"\n",
        "    return X_mat.reshape(orig_shape)\n",
        "\n",
        "\n",
        "def _sym(X):\n",
        "    \"\"\"Return the symmetric part of a square matrix: (X + X^T)/2.\"\"\"\n",
        "    return 0.5 * (X + X.T)\n",
        "\n",
        "def tangent_proj(W, Z):\n",
        "    \"\"\"\n",
        "    Project an ambient matrix/tensor Z onto the tangent space of the Stiefel manifold at W.\n",
        "    Tangent space condition at W: W^T Δ + Δ^T W = 0  (skew-symmetry).\n",
        "    Projection formula: Proj(Z) = Z - W * Sym(W^T Z).\n",
        "\n",
        "    Ici W et Z peuvent être 2D (Linear) ou des tenseurs (Conv2d).\n",
        "    On les reshape en matrice (n, p), on applique la formule, puis on remet en forme.\n",
        "    \"\"\"\n",
        "    orig_shape = W.shape\n",
        "\n",
        "    # Aplatit tout sauf la 1ère dimension : (out_c, in_c, kH, kW) -> (out_c, in_c*kH*kW)\n",
        "    W_mat = W.reshape(W.shape[0], -1)\n",
        "    Z_mat = Z.reshape(Z.shape[0], -1)\n",
        "\n",
        "    n, p = W_mat.shape\n",
        "    if n >= p:\n",
        "        # tall : formule standard\n",
        "        WTZ = W_mat.T @ Z_mat\n",
        "        P_mat = Z_mat - W_mat @ (0.5 * (WTZ + WTZ.T))\n",
        "    else:\n",
        "        # wide : travaille dans l'espace transposé (tall), puis retranspose\n",
        "        Wt, Zt = W_mat.T, Z_mat.T            # shapes: p x n\n",
        "        WtTZt = Wt.T @ Zt                    # n x n\n",
        "        Pt = Zt - Wt @ (0.5 * (WtTZt + WtTZt.T))  # p x n\n",
        "        P_mat = Pt.T                         # n x p\n",
        "\n",
        "    # On revient à la forme originale du poids\n",
        "    return P_mat.reshape(orig_shape)\n",
        "\n",
        "\n",
        "def polar_retraction(X):\n",
        "    \"\"\"\n",
        "    Retract an ambient matrix back to the Stiefel manifold using the polar factor.\n",
        "    This returns the matrix with orthonormal columns closest to X in Frobenius norm.\n",
        "    Prefer torch.linalg.polar when available; fall back to SVD otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # torch >= 2.1: exact polar factorization, numerically stable\n",
        "        U, _ = torch.linalg.polar(X)\n",
        "    except Exception:\n",
        "        # Fallback: X = U Σ V^T  => polar(X) = U V^T\n",
        "        U, _, Vt = torch.linalg.svd(X, full_matrices=False)\n",
        "        U = U @ Vt\n",
        "    return U\n",
        "\n",
        "def transport_proj(W_old, W_new, Xi):\n",
        "    \"\"\"\n",
        "    Vector transport from T_{W_old} to T_{W_new} by simple re-projection.\n",
        "    This is NOT isometric but is widely used in practice: T(Xi) = Proj_{T_{W_new}}(Xi).\n",
        "    Keeps code simple, robust, and compatible with polar retraction.\n",
        "    \"\"\"\n",
        "    return tangent_proj(W_new, Xi)\n",
        "\n",
        "def frob_inner(X, Y):\n",
        "    \"\"\"\n",
        "    Frobenius inner product <X, Y> = trace(X^T Y). Works for same-shaped matrices.\n",
        "    Used to compute curvature scalars (sᵀy), scaling, and two-loop recursion scalars.\n",
        "    \"\"\"\n",
        "    return torch.tensordot(X, Y, dims=([0,1],[0,1]))\n",
        "\n",
        "\n",
        "def retract_qr(X, Xi):  # R_X(Xi)\n",
        "    Y = X + Xi\n",
        "    Q, R = torch.linalg.qr(Y, mode='reduced')\n",
        "    d = torch.sign(torch.diag(R))\n",
        "    D = torch.diag_embed(d)\n",
        "    return Q @ D\n",
        "\n",
        "def project_stiefel_keep_shape_qr(W):\n",
        "    \"\"\"\n",
        "    Project W onto the Stiefel manifold using a QR-based projection,\n",
        "    while preserving the original tensor shape.\n",
        "\n",
        "    For 2D weights, this is the usual QR projection.\n",
        "    For conv weights (e.g. out_c x in_c x kH x kW), we reshape to (m, n),\n",
        "    do the projection, then reshape back.\n",
        "    \"\"\"\n",
        "    W_mat, orig_shape = _as_matrix(W)  # (m, n)\n",
        "    m, n = W_mat.shape\n",
        "\n",
        "    if m >= n:\n",
        "        # tall case\n",
        "        Q, R = torch.linalg.qr(W_mat, mode='reduced')   # Q: m x n\n",
        "        d = torch.sign(torch.diag(R))\n",
        "        D = torch.diag_embed(d)\n",
        "        W_proj = Q @ D                                  # m x n\n",
        "    else:\n",
        "        # wide case: work in the transposed space, then transpose back\n",
        "        Qt, Rt = torch.linalg.qr(W_mat.T, mode='reduced')  # Qt: n x m\n",
        "        d = torch.sign(torch.diag(Rt))\n",
        "        D = torch.diag_embed(d)\n",
        "        W_proj = (Qt @ D).T                                # m x n\n",
        "\n",
        "    return _from_matrix(W_proj, orig_shape)\n",
        "\n",
        "\n",
        "# _retract ensures that W stays a Stiefel\n",
        "@torch.no_grad()\n",
        "def retract_stiefel_shape_preserving(X, Xi):\n",
        "    Y = X + Xi\n",
        "    U, S, Vh = torch.linalg.svd(Y, full_matrices=False)\n",
        "    return U @ Vh  # same shape as X\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO2NxORbbwWT"
      },
      "source": [
        "**Original version from Jeremy Bernstein's paper**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8brQ45AH6S4"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad() #context manager in PyTorch that disables gradient calculation within its scope. This is particularly useful during inference, validation, or when performing operations where you do not intend to update model parameters and therefore do not need to compute gradients.\n",
        "\n",
        "def manifold_muon(W, G, eta=0.1, alpha=0.01, steps=100, tol=1e-6):\n",
        "  '''\n",
        "  W is a 2D pytorch matric, current weights\n",
        "  G is a 2D pytorch matric, loss gradient\n",
        "  eta is the step size for the primal problem\n",
        "  alpha is the step size for the dual problem\n",
        "  steps is the number of steps for the dual problem\n",
        "  tol is the tolerance for the stopping criterion\n",
        "  '''\n",
        "  # Ensure that W and G are both tall matrices (more stable for Stiefel)\n",
        "  should_tranpose = W.shape[0] < W.shape[1]\n",
        "  if should_tranpose:\n",
        "      W = W.T\n",
        "      G = G.T\n",
        "\n",
        "\n",
        "  # Initialize the dual variable\n",
        "  Lambda = -0.25 * (W.T @ G + G.T @ W)\n",
        "\n",
        "\n",
        "  # Ascend on the dual problem to find the update direction A\n",
        "  for step in range(steps):\n",
        "      # Update the candidate direction A\n",
        "      A = msign(G + 2 * W @ Lambda)\n",
        "      # Measure deviation of A from the tangent space:\n",
        "      H = W.T @ A + A.T @ W\n",
        "      # Check the stopping criterion\n",
        "      if torch.norm(H) / math.sqrt(H.numel()) < tol:\n",
        "          break\n",
        "      # Update the dual variable\n",
        "      Lambda -= alpha * (1 - step / steps) * H\n",
        "  # Descend on the primal problem\n",
        "  new_W = W - eta * A\n",
        "  # Retract to the manifold\n",
        "  new_W = msign(new_W)\n",
        "  # Restore the shape of the solution and return\n",
        "  return new_W.T if should_tranpose else new_W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5hdOFPxcRIZ"
      },
      "source": [
        "**Manifold MuOn adaptable for other optimizer instances**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAv6AfqFcgCB"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def manifold_muon_general( W, G, eta=0.1, alpha=0.01, steps=100, tol=1e-6, *, opt = ManifoldLBFGS(eta=0.1, history=10)):\n",
        "    \"\"\"\n",
        "    Drop-in wrapper that replaces the Muon direction with a Riemannian L-BFGS step.\n",
        "\n",
        "    Args:\n",
        "        W (torch.Tensor): current point (n x p), ideally on Stiefel (W^T W ≈ I).\n",
        "        G (torch.Tensor): raw gradient d(loss)/dW, same shape as W.\n",
        "        eta (float): step size; overrides opt.eta for this call.\n",
        "        alpha, steps, tol: kept for signature compatibility (ignored here).\n",
        "        opt (ManifoldLBFGS): persistent optimizer instance.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: new W on the Stiefel manifold (same shape/orientation as input).\n",
        "\n",
        "    Notes:\n",
        "        - This function ONLY performs the STEP. After you recompute the gradient\n",
        "          at the returned W, call opt.update(G_new) once to feed (s_k, y_k) to L-BFGS.\n",
        "        - Removed redundant tall/wide handling from this wrapper; ManifoldLBFGS.step()\n",
        "          now handles it internally and returns W in the original orientation.\n",
        "    \"\"\"\n",
        "    assert opt is not None and W.shape == G.shape, \"Pass your ManifoldLBFGS instance via opt=...\"\n",
        "\n",
        "    # INIT / projection only\n",
        "    if eta == 0.0 or torch.all(G == 0):\n",
        "        W_new = project_stiefel_keep_shape_qr(W)            # <— keeps (out×in) shape\n",
        "        assert W_new.shape == W.shape\n",
        "        return W_new\n",
        "\n",
        "    # Ensure W and G have the same shape\n",
        "    assert W.shape == G.shape, f\"W and G must have the same shape, but got W.shape={W.shape} and G.shape={G.shape}\"\n",
        "\n",
        "    # Set per-step step size (if you want to schedule eta externally, set opt.eta there)\n",
        "    opt.eta = eta\n",
        "    # Cas \"init\": juste projeter W sur Stiefel si eta==0 ou G==0\n",
        "    if eta == 0.0 or torch.all(G == 0):\n",
        "        return retract_qr(W, torch.zeros_like(W))  # projection via QR\n",
        "\n",
        "    # Take one quasi-Newton step on the manifold (retraction included inside .step)\n",
        "    # ManifoldLBFGS.step handles internal transposition and returns W in the original orientation.\n",
        "    new_W = opt.step(W, G)\n",
        "\n",
        "    # The new_W returned by opt.step is already in the original orientation.\n",
        "    return new_W"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}